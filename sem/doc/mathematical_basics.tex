\chapter{Mathematical Basics}
\label{chapter:mathematical_basics}

We give an overview of mathematical notation, which will be used in Section~\ref{chapter:principal_component_analysis} to define PCA. Definitions and theorems, along with their proves are given, ensuring the existence of PCA and showing it is well-defined.

\section{Covariance}
\label{sec:covariance}

Assume we have two sets of $n$ observations of variables with mean $0$. Let us call the first list of observations $\mathbf{a} = (a_1, ..., a_n)^T$ and the second $\mathbf{b} = (b_1, ..., b_n)^T$.

\begin{definition}[Covariance]
	Let us define the \textit{covariance} of $\mathbf{a} \in \mathbbm{R}^n$ and $\mathbf{b} \in \mathbbm{R}^n$ as
	\begin{align*}
		\sigma_{\mathbf{ab}} := \frac{1}{n} \sum_{i=1}^{n}a_ib_i = \frac{1}{n}\mathbf{a}\cdot\mathbf{b}^T.
	\end{align*}
\end{definition}

From the definition it is obvious, that the covariance is symmetric, $\sigma_{\mathbf{ab}} = \sigma_{\mathbf{ba}}$. In the special case $\mathbf{a} = \mathbf{b}$ the covariance $\sigma_{\mathbf{aa}}$ is called \textit{variance} $\sigma_{\mathbf{a}}^2$.

\begin{definition}[Covariance Matrix]
	Generalizing to $m$ variables $\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_m) \in \mathbbm{R}^{n\times m}$, each having been observed $n$ times, gives us the \textit{covariance matrix}.
	
	\begin{align*}
		\mathbf{C_X} := \left(\begin{matrix}
			\sigma_{\mathbf{x_1x_1}}	& \cdots & \sigma_{\mathbf{x_1x_m}}	\\
			\vdots						& \ddots & \vdots					\\
			\sigma_{\mathbf{x_mx_1}}	& \cdots & \sigma_{\mathbf{x_mx_m}}	\\
		\end{matrix}\right) = \frac{1}{n} \mathbf{X}\mathbf{X}^T \in \mathbbm{R}^{m\times m}
	\end{align*}
\end{definition}

The covariance matrix is a symmetric matrix.

\section{Diagonalizable Matrix}
\label{sec:diagonalizable_matrix}

\begin{definition}[Diagonalizable Matrix]
	A square matrix $\mathbf{A} \in \mathbbm{R}^{n \times n}$ is called \textit{diagonalizable}, if there exists an invertable matrix $\mathbf{P} \in \mathbbm{R}^{n \times n}$ and a diagonal matrix $\mathbf{D} \in \mathbbm{R}^{n \times n}$ such that $\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}$.
\end{definition}

\begin{definition}[Symmetric matrix]
	A square matrix $\mathbf{A}$ is called \textit{symmetric}, if $\mathbf{A}^T = \mathbf{A}$.
\end{definition}

\newpage
\begin{theorem}
	\label{th:symmetric_matrix_diagonalizable}
	Every symmetric matrix is diagonalizable.
\end{theorem}

This is the main theorem we need in order to derive PCA. The proof of this theorem requires some preparation, which we will present now.

\begin{definition}[Eigenvalues and Eigenvectors]
	Let $\mathbf{A} \in \mathbbm{R}^{m\times m}$ be a matrix. A number $\lambda \in \mathbbm{C}$ is called a \textit{eigenvalue} with \textit{eigenvector} $\mathbf{v} \in \mathbbm{C}^m\setminus\{\mathbf{0}\}$ if
	\begin{align}
		\label{eq:def_eigenvalue}
		\mathbf{Av} = \lambda \mathbf{v}.
	\end{align}
\end{definition}

\begin{lemma}
	\label{lem:existence_eigenvalues}
	Every square matrix $\mathbf{A} \in \mathbbm{R}^{m\times m}$ has $m$ (not necessarily unique) eigenvalues.
\end{lemma}

\begin{proof}
	We can rewrite equation~\ref{eq:def_eigenvalue} as
	\begin{align*}
		(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = \mathbf{0}.
	\end{align*}
	
	This allows us to interpret $(\mathbf{A}-\lambda \mathbf{I})$ as a function, which takes vectors $\mathbf{v} \in \mathbbm{C}^m$. For $\lambda \in \mathbbm{C}$ to be a eigenvalue of $\mathbf{A}$ with eigenvector $\mathbf{v}$ it has to satisfy $\mathbf{v} \in \ker(\mathbf{A} - \lambda \mathbf{I})$ and $\mathbf{v} \neq 0$. From this we gather that all $\lambda$ with $\dim(\ker(\mathbf{A} - \lambda \mathbf{I})) > 0$ are eigenvalues. We know this holds if and only if $\det(\mathbf{A} - \lambda \mathbf{I}) = 0$. The determinant is a polynomial of degree $m$ which can be expressed in the form $(\lambda - \lambda_1)...(\lambda - \lambda_m)$ with $\lambda_1, ..., \lambda_m \in \mathbbm{C}$. These $\lambda_1, ..., \lambda_m$ are the $m$ eigenvalues we wanted to find.
\end{proof}

\begin{lemma}
	A symmetric matrix has real eigenvalues.
\end{lemma}

\begin{proof}
	Let $\bar{.}$ denote the complex conjugate. Define a complex dot product
	\begin{align*}
		(\mathbf{u}, \mathbf{v}) := \sum_{i=1}^{m} u_i \bar{v_i}.
	\end{align*}
	This dot product has the following properties for all $\mathbf{A} \in \mathbbm{C}^{m\times m}, \mathbf{u}, \mathbf{v} \in \mathbbm{C}^m, \lambda \in \mathbbm{C}$.
	
	\begin{align*}
		(\mathbf{Au}, \mathbf{v}) = (\mathbf{u}, \mathbf{A}^T\mathbf{v}), && (\lambda \mathbf{u}, \mathbf{v}) = \lambda(\mathbf{u}, \mathbf{v}), && (\mathbf{u}, \lambda \mathbf{v}) = \bar{\lambda} (\mathbf{u}, \mathbf{v}), && (\mathbf{u}, \mathbf{u}) = 0 \iff \mathbf{u} = 0.
	\end{align*}
	
	Let $\mathbf{A}$ be a symmetric matrix with eigenvalue $\lambda \in \mathbbm{C}$.
	
	For all $\mathbf{u} \in \mathbbm{C}^m$ we have
	\begin{align*}
		\lambda (\mathbf{u}, \mathbf{u}) = (\lambda \mathbf{u}, \mathbf{u}) = (\mathbf{Au}, \mathbf{u}) = (\mathbf{u}, \mathbf{A}^T\mathbf{u}) =\\
		(\mathbf{u}, \mathbf{Au}) =	(\mathbf{u}, \lambda\mathbf{u}) = \bar{\lambda} (\mathbf{u}, \mathbf{u}).
	\end{align*}
	
	For $\mathbf{u} \neq \mathbf{0}$ we get $\lambda = \bar{\lambda}$ and thus $\lambda \in \mathbbm{R}$.
\end{proof}

Are the corresponding eigenvectors real? From the proof of lemma~\ref{lem:existence_eigenvalues} we know that the eigenvector $\mathbf{v}$ of eigenvalue $\lambda$ is in $\ker(\mathbf{A} - \lambda\mathbf{I})$. Both the matrix $\mathbf{A}$ and $\lambda$ are real, so $\mathbf{v}$ must be in $\mathbbm{R}^m$ as well.

\begin{lemma}
	\label{lem:symmetric_matrix_eigenvector_orthogonal}
	The eigenvectors of a symmetric matrix with distinct eigenvalues are orthogonal.
\end{lemma}

\begin{proof}
	Let $\lambda_1, \lambda_2$ be two distinct eigenvalues with eigenvectors $\mathbf{v}_1, \mathbf{v}_2$ of the matrix $\mathbf{A}$ respectively. It holds that
	
	\begin{align*}
		\lambda_1 \mathbf{v}_1 \cdot \mathbf{v}_2 = (\lambda_1\mathbf{v}_1)^T \mathbf{v}_2 = (\mathbf{Av}_1)^T \mathbf{v}_2 = \mathbf{v}_1^T \mathbf{A}^T \mathbf{v}_2 =\\
		\mathbf{v}_1^T \mathbf{A} \mathbf{v}_2 = \mathbf{v}_1^T (\lambda_2 \mathbf{v}_2) = \lambda_2 \mathbf{v}_1 \cdot \mathbf{v}_2
	\end{align*}
	
	This shows $(\lambda_1 - \lambda_2) \mathbf{v}_1 \cdot \mathbf{v}_2 = 0$ and as $\lambda_1$ and $\lambda_2$ are distinct, $\mathbf{v}_1$ and $\mathbf{v}_2$ must be orthogonal.
\end{proof}

What if the eigenvalues of the matrix are not distinct? In the proof of lemma~\ref{lem:existence_eigenvalues} we showed that every $\mathbf{v} \in \ker(\mathbf{A} - \lambda\mathbf{I}) \setminus \{\mathbf{0}\}$ is an eigenvector. If and only if $(\lambda - \lambda_i)$ appears $k \geq 2$ times in the determinant of $(\mathbf{A} - \lambda\mathbf{I})$ then $\mathbf{A}$ has a non unique eigenvalue $\lambda_i$. As ${\dim(\ker(\mathbf{A} - \lambda_i\mathbf{I})) = k}$ we can choose orthogonal eigenvectors. 


Now we have everything we need to prove theorem~\ref{th:symmetric_matrix_diagonalizable}. 

\begin{proof}[Proof of Theorem~\ref{th:symmetric_matrix_diagonalizable}]
	Let $\mathbf{A} \in \mathbbm{R}^{m\times m}$ be a symmetric matrix. From lemma~\ref{lem:existence_eigenvalues} we know that eigenvalues $\lambda_1, ..., \lambda_m$ with corresponding eigenvectors $\mathbf{v}_1, ..., \mathbf{v}_m$ exist.
	
	Define the following matrices
	\begin{align*}
		\mathbf{D} := \left(\begin{matrix}
			\lambda_1 & 0 & \cdots & 0\\
			0 & \lambda_2 & \cdots & 0\\
			\vdots & \vdots & \ddots & \vdots\\
			0 & 0 & \cdots & \lambda_m
		\end{matrix}\right), &&
		\mathbf{V} := \left(\begin{matrix}
			& & &\\
			\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_m\\
			& & &\\
		\end{matrix}\right).
	\end{align*}
	
	The definition of eigenvalues and eigenvectors gives us
	\begin{align}
		\label{eq:diagonalizable}
		\mathbf{AV} = \left(\begin{matrix}
			\mathbf{Av}_1 & \cdots & \mathbf{Av}_m
		\end{matrix}\right) = \left(\begin{matrix}
			\lambda_1\mathbf{v}_1 & \cdots & \lambda_m\mathbf{v}_m
		\end{matrix}\right) = \mathbf{VD}.
	\end{align}
	
	From lemma~\ref{lem:symmetric_matrix_eigenvector_orthogonal} we know that the eigenvectors, and therefore the columns of $\mathbf{V}$, are orthogonal. It follows that rank$(\mathbf{V}) = m$ which gives us the existence of $\mathbf{V}^{-1}$.
	
	Rearranging equation~\ref{eq:diagonalizable} now gives us $\mathbf{A} = \mathbf{VDV}^{-1}$ which is what we wanted to show.
	
	This shows that $\mathbf{A}$ is diagonalizable.
\end{proof}


\newpage
\begin{lemma}
	\label{lem:inverse_is_transpose}
	If the columns of matrix $\mathbf{A}$ are orthonormal, then $\mathbf{A}^{-1} = \mathbf{A}^T$.
\end{lemma}

\begin{proof}
	Let $(\mathbf{a}_i)_{i=1,...,m}$ be the columns of the matrix. The columns are orthogonal and normed, therefore
	
	\begin{align*}
		\forall i,j: \mathbf{a}_i^T\mathbf{a}_j = \begin{cases}
			1, & \text{if } i=j,\\
			0, & \text{otherwise.}
		\end{cases} \implies \mathbf{A}^T\mathbf{A} = \mathbf{I}
	\end{align*}
	
	This shows $\mathbf{A}^{-1} = \mathbf{A}^T$.
\end{proof}