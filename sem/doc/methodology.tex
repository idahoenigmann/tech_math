For the algorithm in section~\ref{chapter:data_and_algorithm} a few other methods will be used. As they are not the focus of this work we only give a short overview.

\section{Fourier Transformation}
\label{sec:fourier_transformation}

Many applications are concerned with cyclic temporal data. Examples are sound waves or EEG data. This data can be represented as a function of amplitude over time. Most often we are not interested at the amplitudes at a specific point in time, as a temporal shift would represent a very similar information. This is demonstrated in figure~\ref{fig:TOOD}.

As the function is cyclic we might be interested in a decomposition into simple cyclic function, such as $\sin$. We can then analyze the most prominent frequencies and their respective amplitudes. This gives a representation of the data, that can be easier to interpret.

We describe one approach to achieve this representation as a sum of $\sin$ functions, called Fast Fourier Transformation (FFT), in algorithm~\ref{alg:fft}.

\begin{algorithm}
	\caption{FFT}\label{alg:fft}
	\begin{algorithmic}
		\Function{FFT}{$x$, $N$, $s$}
			\If{$N$ = $1$}
				\State $X_0 \gets x_0$
			\Else
				\State $X_{0, ..., N/2-1} \gets FFT(x, N/2, 2s)$
				\State $X_{N/2, ..., N-1} \gets FFT(x+s, N/2, 2s)$
				\For{$k=0$ to $N/2-1$}
					\State $p \gets X_k$
					\State $q \gets exp(-2\pi i/N k) X_{k+N/2}$
					\State $X_k \gets p + q$
					\State $X_{k+N/2} \gets p - q$
				\EndFor
			\EndIf
			\State \Return $X_{0, ..., N-1}$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Figure~\ref{fig:TODO} shows how an example input and output of the FFT algorithm looks.

\section{Classification}
\label{sec:classification}

In classification the objective is to find assignments between data points and categories. In our context we are interested in finding an assignment which closely matches some already categorized data. One simple approach to this problem is the k-nearest-neighbors algorithm.

For data that can be represented in $\mathbbm{R}^m$ and $l$ categories the pseudo code is shown in algorithm~\ref{alg:k_nearest_neighbors}.

\begin{algorithm}
	\caption{k Nearest Neighbors}\label{alg:k_nearest_neighbors}
	\begin{algorithmic}
		\Require data points $(p_i)_{i \in \mathbbm{N}} \in \mathbbm{R}^{m}$, categories $(c_i)_{i \in \mathbbm{N}} \in \{0, 1, ..., l\}$, point $x \in \mathbbm{R}^{m}$, $k \in \mathbbm{N}$
		\State Calculate the distance between each point $p_i$ and $x$
		\State Take the $k$ data points with the smallest distance to $x$
		\State Return the category that most of the $k$ points are assigned
	\end{algorithmic}
\end{algorithm}

Figure~\ref{fig:k_nearest_neighbors} shows a graphical representation of the algorithm for points in $\mathbbm{R}^2$ and $k=10$.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figs/k_nearest_neighbors}
	\caption{Data points in three categories are given. The black point is classified as a blue, circular point by the k-nearest-neighbors algorithm for $k=10$.}
	\label{fig:k_nearest_neighbors}
\end{figure}

This algorithm is slow for big datasets as for each point the distance to $x$ has to be calculated. One possibility to reduce calculation time is to partition space into smaller chunks. Then the loop only has to be over data points lying in the same or close chunks as the point we are interested in.
